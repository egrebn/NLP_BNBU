{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edict Similarity Analysis with SIKU-BERT\n",
    "\n",
    "This notebook analyzes edicts of a specified type from the extracted_edicts_punc.csv file by creating embedd}ings using SIKU-BERT and comparing their content similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Configure proxy settings and install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy torch transformers scikit-learn matplotlib seaborn tqdm scipy accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import the necessary libraries, including pandas for data manipulation, transformers for SIKU-BERT, and matplotlib/seaborn for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "plt.rcParams['figure.figsize'] = (12, 10)\n",
    "\n",
    "# Configure Chinese font support for all platforms\n",
    "print(\"Configuring Chinese font support...\")\n",
    "available_fonts = [f.name for f in fm.fontManager.ttflist]\n",
    "\n",
    "# Comprehensive list of Chinese fonts for different platforms\n",
    "# Windows: SimHei, Microsoft YaHei, SimSun\n",
    "# Mac: PingFang SC, Heiti SC, STHeiti\n",
    "# Linux: WenQuanYi, Noto Sans CJK, Droid Sans Fallback\n",
    "# Colab/Cloud: Noto Sans CJK SC (pre-installed)\n",
    "chinese_fonts = [\n",
    "    'Noto Sans CJK SC', 'Noto Sans CJK TC', 'Noto Sans SC',  # Cloud/Linux priority\n",
    "    'PingFang SC', 'Heiti SC', 'STHeiti',  # Mac\n",
    "    'Microsoft YaHei', 'SimHei', 'SimSun',  # Windows\n",
    "    'WenQuanYi Micro Hei', 'WenQuanYi Zen Hei',  # Linux\n",
    "    'Droid Sans Fallback', 'AR PL UMing CN'\n",
    "]\n",
    "\n",
    "selected_font = None\n",
    "for font in chinese_fonts:\n",
    "    if font in available_fonts:\n",
    "        selected_font = font\n",
    "        print(f\"✓ Found Chinese font: {selected_font}\")\n",
    "        break\n",
    "\n",
    "if selected_font is None:\n",
    "    print(\"⚠ No Chinese font found. Chinese characters may not display correctly.\")\n",
    "    print(\"  On Colab, run: !apt-get install -y fonts-noto-cjk\")\n",
    "    print(\"  On Linux, run: sudo apt-get install fonts-noto-cjk\")\n",
    "    print(\"  On Mac/Windows, Chinese fonts should be pre-installed.\")\n",
    "    selected_font = 'DejaVu Sans'  # Fallback\n",
    "else:\n",
    "    # Configure matplotlib to use the selected font globally\n",
    "    plt.rcParams['font.sans-serif'] = [selected_font, 'DejaVu Sans']\n",
    "    plt.rcParams['axes.unicode_minus'] = False  # Fix minus sign display\n",
    "    print(f\"✓ Matplotlib configured to use: {selected_font}\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load SIKU-BERT Model\n",
    "\n",
    "Load the SIKU-BERT model and tokenizer from a local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration - using local model directory\n",
    "model_id = \"sikubert\"\n",
    "\n",
    "print(\"Loading SIKU-BERT tokenizer and model from local directory...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "print(\"Loading SIKU-BERT model...\")\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    output_attentions=True,  # Enable attention output for token weighting\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(\"SIKU-BERT model loaded successfully!\")\n",
    "print(f\"Model device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Embedding Function\n",
    "\n",
    "Create a function to generate embeddings using SIKU-BERT with mean pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, pooling='mean'):\n",
    "    \"\"\"\n",
    "    Generate embedding for text using SIKU-BERT.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to embed\n",
    "        pooling: Pooling strategy ('mean', 'max', 'cls')\n",
    "    \n",
    "    Returns:\n",
    "        Numpy array of embedding\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, \n",
    "                      truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "    \n",
    "    # Apply pooling\n",
    "    if pooling == 'mean':\n",
    "        # Mean pooling\n",
    "        attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "        masked_embeddings = embeddings * attention_mask\n",
    "        sum_embeddings = masked_embeddings.sum(dim=1)\n",
    "        sum_mask = attention_mask.sum(dim=1)\n",
    "        pooled = sum_embeddings / sum_mask\n",
    "    elif pooling == 'max':\n",
    "        # Max pooling\n",
    "        pooled = torch.max(embeddings, dim=1)[0]\n",
    "    else:  # 'cls'\n",
    "        # Use [CLS] token\n",
    "        pooled = embeddings[:, 0, :]\n",
    "    \n",
    "    return pooled.cpu().numpy()[0]\n",
    "\n",
    "print(\"Embedding function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Filter the Data\n",
    "\n",
    "Load the CSV file extracted_edicts_punctuated.csv using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('extracted_edicts_punc.csv', encoding='utf-8-sig')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Total rows in dataset: {len(df)}\")\n",
    "print(f\"\\nColumn names: {df.columns.tolist()}\")\n",
    "\n",
    "# Display unique document types\n",
    "if 'document_type' in df.columns:\n",
    "    unique_types = df['document_type'].unique()\n",
    "    print(f\"\\nUnique document types ({len(unique_types)}):\")\n",
    "    for doc_type in sorted(unique_types):\n",
    "        count = len(df[df['document_type'] == doc_type])\n",
    "        print(f\"  - {doc_type}: {count} documents\")\n",
    "else:\n",
    "    print(\"\\nNo document_type column found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Configure Document Type\n",
    "\n",
    "Select which document type to analyze. You can change this to any of the types listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the document type to analyze\n",
    "DOCUMENT_TYPE = '恩宥'  # Change this to analyze a different document type\n",
    "\n",
    "# Filter for the selected document type\n",
    "df_d_type = df[df['document_type'] == DOCUMENT_TYPE].copy()\n",
    "print(f\"\\nFiltered for document type: '{DOCUMENT_TYPE}'\")\n",
    "print(f\"Number of documents: {len(df_d_type)}\")\n",
    "\n",
    "# Display first few rows\n",
    "df_d_type.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocess the Text Data\n",
    "\n",
    "Clean and preprocess the text_contents_punctuated column by removing null values and performing any necessary text cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "print(f\"Null values in text_contents_punctuated: {df_d_type['text_contents_punctuated'].isnull().sum()}\")\n",
    "\n",
    "# Remove rows with null text content\n",
    "df_d_type = df_d_type[df_d_type['text_contents_punctuated'].notna()].copy()\n",
    "\n",
    "# Remove rows with empty strings\n",
    "df_d_type = df_d_type[df_d_type['text_contents_punctuated'].str.strip() != ''].copy()\n",
    "\n",
    "# Reset index\n",
    "df_d_type.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"\\nNumber of edicts after cleaning: {len(df_d_type)}\")\n",
    "\n",
    "# Display text length statistics\n",
    "df_d_type['text_length'] = df_d_type['text_contents_punctuated'].str.len()\n",
    "print(f\"\\nText length statistics:\")\n",
    "print(df_d_type['text_length'].describe())\n",
    "\n",
    "# Display sample texts\n",
    "print(\"\\nSample edict texts:\")\n",
    "for idx, text in enumerate(df_d_type['text_contents_punctuated'].head(3)):\n",
    "    print(f\"\\nEdict {idx + 1}: {text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Text Embeddings with SIKU-BERT\n",
    "\n",
    "Use SIKU-BERT to convert the text contents into numerical embeddings/vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all edicts\n",
    "print(\"Generating embeddings for all edicts using SIKU-BERT...\")\n",
    "print(\"This may take a few minutes depending on the number of edicts.\\n\")\n",
    "\n",
    "embeddings_list = []\n",
    "\n",
    "for idx, text in enumerate(tqdm(df_d_type['text_contents_punctuated'], desc=\"Encoding texts\")):\n",
    "    embedding = get_embedding(text, pooling='mean')\n",
    "    embeddings_list.append(embedding)\n",
    "\n",
    "# Convert to numpy array\n",
    "edict_embeddings = np.array(embeddings_list)\n",
    "\n",
    "print(f\"\\nEmbedding shape: {edict_embeddings.shape}\")\n",
    "print(f\"Number of edicts: {edict_embeddings.shape[0]}\")\n",
    "print(f\"Embedding dimension: {edict_embeddings.shape[1]}\")\n",
    "\n",
    "# Save embeddings for later use\n",
    "np.save(f'{DOCUMENT_TYPE}_embeddings_sikubert.npy', edict_embeddings)\n",
    "print(f\"\\nEmbeddings saved to '{DOCUMENT_TYPE}_embeddings_sikubert.npy'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calculate Similarity Between Edicts\n",
    "\n",
    "Compute pairwise cosine similarity between all edict embeddings to measure content similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity matrix\n",
    "print(\"Calculating pairwise cosine similarity...\")\n",
    "similarity_matrix = cosine_similarity(edict_embeddings)\n",
    "\n",
    "print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "print(f\"\\nSimilarity statistics:\")\n",
    "upper_triangle = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]\n",
    "print(f\"Min similarity: {upper_triangle.min():.4f}\")\n",
    "print(f\"Max similarity (excluding diagonal): {upper_triangle.max():.4f}\")\n",
    "print(f\"Mean similarity: {upper_triangle.mean():.4f}\")\n",
    "print(f\"Median similarity: {np.median(upper_triangle):.4f}\")\n",
    "print(f\"Std deviation: {upper_triangle.std():.4f}\")\n",
    "\n",
    "# Display a sample of the similarity matrix\n",
    "print(\"\\nSample of similarity matrix (first 5x5):\")\n",
    "print(pd.DataFrame(similarity_matrix[:5, :5]).round(3))\n",
    "\n",
    "# Save similarity matrix\n",
    "np.save(f'{DOCUMENT_TYPE}_similarity_matrix_sikubert.npy', similarity_matrix)\n",
    "print(f\"\\nSimilarity matrix saved to '{DOCUMENT_TYPE}_similarity_matrix_sikubert.npy'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Similarity Matrix\n",
    "\n",
    "Create a heatmap visualization of the similarity matrix to identify patterns and clusters of similar edicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of similarity matrix with edict titles\n",
    "# Prepare labels - truncate long titles for readability\n",
    "labels = []\n",
    "for i in range(len(df_d_type)):\n",
    "    title = df_d_type.iloc[i]['text_title'] if 'text_title' in df_d_type.columns else f\"Edict {i}\"\n",
    "    # Truncate title if too long\n",
    "    if len(title) > 20:\n",
    "        title = title[:20] + '...'\n",
    "    # Add index for clarity\n",
    "    labels.append(f\"{i}: {title}\")\n",
    "\n",
    "# Determine appropriate font size based on number of edicts\n",
    "n_edicts = len(df_d_type)\n",
    "if n_edicts <= 20:\n",
    "    label_font_size = 8\n",
    "    fig_size = (16, 14)\n",
    "elif n_edicts <= 50:\n",
    "    label_font_size = 6\n",
    "    fig_size = (20, 18)\n",
    "else:\n",
    "    label_font_size = 4\n",
    "    fig_size = (24, 22)\n",
    "\n",
    "# Ensure Chinese font is being used for this plot\n",
    "try:\n",
    "    current_font = plt.rcParams['font.sans-serif'][0]\n",
    "    print(f\"Using font for heatmap: {current_font}\")\n",
    "except:\n",
    "    print(\"⚠ Font not configured, Chinese characters may not display correctly\")\n",
    "\n",
    "plt.figure(figsize=fig_size)\n",
    "sns.heatmap(\n",
    "    similarity_matrix,\n",
    "    cmap='YlOrRd',\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    square=True,\n",
    "    cbar_kws={'label': 'Cosine Similarity'},\n",
    "    xticklabels=labels,\n",
    "    yticklabels=labels\n",
    ")\n",
    "plt.title(f'Cosine Similarity Matrix of {DOCUMENT_TYPE} Edicts (SIKU-BERT)', fontsize=16, pad=20)\n",
    "plt.xlabel('Edict Index and Title', fontsize=12)\n",
    "plt.ylabel('Edict Index and Title', fontsize=12)\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=90, fontsize=label_font_size)\n",
    "plt.yticks(rotation=0, fontsize=label_font_size)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'similarity_heatmap_{DOCUMENT_TYPE}_sikubert.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Heatmap saved as 'similarity_heatmap_{DOCUMENT_TYPE}_sikubert.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a distribution plot of similarity scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "similarity_scores = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]\n",
    "plt.hist(similarity_scores, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "plt.axvline(similarity_scores.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {similarity_scores.mean():.3f}')\n",
    "plt.axvline(np.median(similarity_scores), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(similarity_scores):.3f}')\n",
    "plt.xlabel('Cosine Similarity Score', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title(f'Distribution of Pairwise Similarity Scores (SIKU-BERT) - {DOCUMENT_TYPE}', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'similarity_distribution_{DOCUMENT_TYPE}_sikubert.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Distribution plot saved as 'similarity_distribution_{DOCUMENT_TYPE}_sikubert.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Find Most Similar Edicts\n",
    "\n",
    "For each edict, identify and display the top N most similar edicts based on the similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find top N most similar edicts\n",
    "def find_most_similar(edict_idx, similarity_matrix, df, top_n=5):\n",
    "    \"\"\"\n",
    "    Find the top N most similar edicts to a given edict.\n",
    "    \n",
    "    Parameters:\n",
    "    - edict_idx: Index of the edict to compare\n",
    "    - similarity_matrix: Pairwise similarity matrix\n",
    "    - df: DataFrame containing edict information\n",
    "    - top_n: Number of similar edicts to return\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with most similar edicts\n",
    "    \"\"\"\n",
    "    # Get similarity scores for the given edict\n",
    "    similarities = similarity_matrix[edict_idx]\n",
    "    \n",
    "    # Get indices of top N similar edicts (excluding itself)\n",
    "    similar_indices = np.argsort(similarities)[::-1][1:top_n+1]\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results = []\n",
    "    for rank, idx in enumerate(similar_indices, 1):\n",
    "        results.append({\n",
    "            'Rank': rank,\n",
    "            'Index': idx,\n",
    "            'Similarity': similarities[idx],\n",
    "            'Title': df.iloc[idx]['text_title'] if 'text_title' in df.columns else f\"Edict {idx}\",\n",
    "            'Text_Preview': df.iloc[idx]['text_contents_punctuated'][:100] + '...'\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example: Find most similar edicts for the first 3 edicts\n",
    "print(\"=\" * 100)\n",
    "for i in range(min(3, len(df_d_type))):\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    title = df_d_type.iloc[i]['text_title'] if 'text_title' in df_d_type.columns else f\"Edict {i}\"\n",
    "    print(f\"EDICT #{i} - {title}\")\n",
    "    print(f\"Original Text:\")\n",
    "    print(f\"{df_d_type.iloc[i]['text_contents_punctuated'][:200]}...\")\n",
    "    print(f\"\\n{'-'*100}\")\n",
    "    print(f\"Top 5 Most Similar Edicts:\")\n",
    "    print(f\"{'-'*100}\")\n",
    "    \n",
    "    similar_edicts = find_most_similar(i, similarity_matrix, df_d_type, top_n=5)\n",
    "    \n",
    "    for _, row in similar_edicts.iterrows():\n",
    "        print(f\"\\nRank {row['Rank']} - {row['Title']} (Similarity: {row['Similarity']:.4f}):\")\n",
    "        print(f\"  {row['Text_Preview']}\")\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Edict Similarity Summary Statistics\n",
    "\n",
    "Calculate and display summary statistics for each edict's similarity to all other edicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary DataFrame with similarity statistics for each edict\n",
    "summary_data = []\n",
    "\n",
    "for i in range(len(df_d_type)):\n",
    "    similarities = similarity_matrix[i]\n",
    "    # Exclude self-similarity (diagonal)\n",
    "    other_similarities = np.concatenate([similarities[:i], similarities[i+1:]])\n",
    "    \n",
    "    title = df_d_type.iloc[i]['text_title'] if 'text_title' in df_d_type.columns else f\"Edict {i}\"\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Edict_Index': i,\n",
    "        'Title': title,\n",
    "        'Text_Length': df_d_type.iloc[i]['text_length'],\n",
    "        'Text_Preview': df_d_type.iloc[i]['text_contents_punctuated'][:80] + '...',\n",
    "        'Avg_Similarity': other_similarities.mean(),\n",
    "        'Max_Similarity': other_similarities.max(),\n",
    "        'Min_Similarity': other_similarities.min(),\n",
    "        'Std_Similarity': other_similarities.std()\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "# Set Edict_Index as the index to avoid duplicate numbering\n",
    "summary_df.set_index('Edict_Index', inplace=True)\n",
    "\n",
    "print(\"\\nEdicts Ranked by Average Similarity to Other Edicts:\")\n",
    "print(\"=\"*100)\n",
    "summary_df.sort_values('Avg_Similarity', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10b. Identify Highly Similar Pairs\n",
    "\n",
    "Find pairs of edicts with similarity scores above a specified threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify pairs of highly similar edicts\n",
    "# Start with a high threshold and provide statistics to guide adjustment\n",
    "threshold = 0.98  # Adjust this threshold as needed (try 0.9, 0.95, or 0.98)\n",
    "\n",
    "print(f\"Pairs of edicts with similarity >= {threshold}:\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "high_similarity_pairs = []\n",
    "for i in range(len(similarity_matrix)):\n",
    "    for j in range(i+1, len(similarity_matrix)):\n",
    "        if similarity_matrix[i, j] >= threshold:\n",
    "            title_1 = df_d_type.iloc[i]['text_title'] if 'text_title' in df_d_type.columns else f\"Edict {i}\"\n",
    "            title_2 = df_d_type.iloc[j]['text_title'] if 'text_title' in df_d_type.columns else f\"Edict {j}\"\n",
    "            \n",
    "            high_similarity_pairs.append({\n",
    "                'Edict_1_Index': i,\n",
    "                'Edict_1_Title': title_1,\n",
    "                'Edict_2_Index': j,\n",
    "                'Edict_2_Title': title_2,\n",
    "                'Similarity': similarity_matrix[i, j],\n",
    "                'Text_1_Preview': df_d_type.iloc[i]['text_contents_punctuated'][:80] + '...',\n",
    "                'Text_2_Preview': df_d_type.iloc[j]['text_contents_punctuated'][:80] + '...'\n",
    "            })\n",
    "\n",
    "if high_similarity_pairs:\n",
    "    pairs_df = pd.DataFrame(high_similarity_pairs)\n",
    "    pairs_df = pairs_df.sort_values('Similarity', ascending=False)\n",
    "    # Reset index to show Pair_Number starting from 1\n",
    "    pairs_df.reset_index(drop=True, inplace=True)\n",
    "    pairs_df.index = pairs_df.index + 1\n",
    "    pairs_df.index.name = 'Pair_Number'\n",
    "    \n",
    "    print(f\"\\nFound {len(pairs_df)} pairs with similarity >= {threshold}\")\n",
    "    \n",
    "    # Show threshold guidance\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"Threshold Guidance:\")\n",
    "    print(\"=\"*100)\n",
    "    for test_threshold in [0.99, 0.95, 0.90, 0.85, 0.80]:\n",
    "        count = len([1 for i in range(len(similarity_matrix)) \n",
    "                     for j in range(i+1, len(similarity_matrix)) \n",
    "                     if similarity_matrix[i, j] >= test_threshold])\n",
    "        print(f\"  Threshold {test_threshold}: {count} pairs\")\n",
    "    print(\"\\nRecommendation: Use a threshold that gives 10-100 pairs for meaningful analysis\")\n",
    "    print(\"=\"*100 + \"\\n\")\n",
    "    \n",
    "    # Save to CSV for further analysis\n",
    "    pairs_df.to_csv(f'similar_pairs_{DOCUMENT_TYPE}_threshold_{threshold}.csv', \n",
    "                    encoding='utf-8-sig')\n",
    "    print(f\"Pairs saved to 'similar_pairs_{DOCUMENT_TYPE}_threshold_{threshold}.csv'\\n\")\n",
    "    \n",
    "    # Display the results\n",
    "    print(f\"Displaying top 20 most similar pairs:\")\n",
    "    display(pairs_df.head(20))\n",
    "    \n",
    "    # Return the full dataframe for further inspection\n",
    "    pairs_df\n",
    "else:\n",
    "    print(f\"\\nNo pairs found with similarity >= {threshold}\")\n",
    "    print(\"Consider lowering the threshold to find similar pairs.\")\n",
    "    print(f\"\\nTry thresholds between {upper_triangle.mean():.3f} (mean) and {upper_triangle.max():.3f} (max)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cluster Analysis \n",
    "\n",
    "Perform hierarchical clustering to identify groups of similar edicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Font was already configured in section 1, just verify it's available\n",
    "print(\"Using configured Chinese font for dendrogram...\")\n",
    "try:\n",
    "    current_font = plt.rcParams['font.sans-serif'][0]\n",
    "    print(f\"✓ Current font: {current_font}\")\n",
    "    selected_font = current_font\n",
    "except:\n",
    "    # Fallback if font wasn't configured\n",
    "    selected_font = 'DejaVu Sans'\n",
    "    print(\"⚠ Using fallback font: DejaVu Sans\")\n",
    "\n",
    "# Convert similarity to distance\n",
    "distance_matrix = 1 - similarity_matrix\n",
    "\n",
    "# Ensure the matrix is symmetric and diagonal is zero\n",
    "distance_matrix = (distance_matrix + distance_matrix.T) / 2\n",
    "np.fill_diagonal(distance_matrix, 0)\n",
    "\n",
    "# Convert the square distance matrix to condensed form (upper triangle)\n",
    "# scipy's linkage expects a condensed distance matrix\n",
    "condensed_distance = squareform(distance_matrix, checks=False)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "print(\"Performing hierarchical clustering...\")\n",
    "linkage_matrix = linkage(condensed_distance, method='average')\n",
    "\n",
    "# Create labels from edict titles and social categories\n",
    "# Truncate long titles and add index for clarity\n",
    "labels = []\n",
    "for i in range(len(df_d_type)):\n",
    "    title = df_d_type.iloc[i]['text_title'] if 'text_title' in df_d_type.columns else f\"Edict {i}\"\n",
    "    social_cat = df_d_type.iloc[i]['social_category'] if 'social_category' in df_d_type.columns else ''\n",
    "    \n",
    "    # Truncate title if too long\n",
    "    if len(title) > 15:\n",
    "        title = title[:15] + '...'\n",
    "    \n",
    "    # Combine index, social category, and title\n",
    "    if social_cat:\n",
    "        labels.append(f\"{i}: [{social_cat}] {title}\")\n",
    "    else:\n",
    "        labels.append(f\"{i}: {title}\")\n",
    "\n",
    "# Plot dendrogram with Chinese character support\n",
    "plt.figure(figsize=(24, 12))\n",
    "dendrogram(\n",
    "    linkage_matrix, \n",
    "    labels=labels, \n",
    "    leaf_font_size=8,\n",
    "    leaf_rotation=90,  # Rotate labels vertically for better readability\n",
    "    color_threshold=0.7 * max(linkage_matrix[:, 2])  # Color threshold for visual grouping\n",
    ")\n",
    "plt.title(f'{DOCUMENT_TYPE}诏令层次聚类分析 (SIKU-BERT)', fontsize=18, pad=20, \n",
    "         fontproperties=fm.FontProperties(family=selected_font))\n",
    "plt.xlabel('诏令编号、类别与标题', fontsize=14, \n",
    "          fontproperties=fm.FontProperties(family=selected_font))\n",
    "plt.ylabel('距离 (1 - 相似度)', fontsize=14, \n",
    "          fontproperties=fm.FontProperties(family=selected_font))\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'dendrogram_{DOCUMENT_TYPE}_sikubert.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dendrogram saved as 'dendrogram_{DOCUMENT_TYPE}_sikubert.png'\")\n",
    "\n",
    "# Save the mapping table to a CSV file for easy reference\n",
    "mapping_df = pd.DataFrame({\n",
    "    'Index': range(len(df_d_type)),\n",
    "    'Social_Category': [df_d_type.iloc[i]['social_category'] if 'social_category' in df_d_type.columns \n",
    "                        else '' for i in range(len(df_d_type))],\n",
    "    'Title': [df_d_type.iloc[i]['text_title'] if 'text_title' in df_d_type.columns \n",
    "              else f\"Edict {i}\" for i in range(len(df_d_type))],\n",
    "    'Text_Preview': [df_d_type.iloc[i]['text_contents_punctuated'][:100] + '...' \n",
    "                     for i in range(len(df_d_type))]\n",
    "})\n",
    "\n",
    "mapping_df.to_csv(f'edict_index_mapping_{DOCUMENT_TYPE}.csv', index=False, encoding='utf-8')\n",
    "print(f\"\\nEdict index mapping saved to 'edict_index_mapping_{DOCUMENT_TYPE}.csv'\")\n",
    "\n",
    "# Also print the mapping table\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"Edict Index to Title Mapping:\")\n",
    "print(\"=\"*100)\n",
    "for i in range(len(df_d_type)):\n",
    "    title = df_d_type.iloc[i]['text_title'] if 'text_title' in df_d_type.columns else f\"Edict {i}\"\n",
    "    social_cat = df_d_type.iloc[i]['social_category'] if 'social_category' in df_d_type.columns else ''\n",
    "    if social_cat:\n",
    "        print(f\"{i}: [{social_cat}] {title}\")\n",
    "    else:\n",
    "        print(f\"{i}: {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has successfully:\n",
    "1. Loaded and filtered edicts of the selected document type from the dataset\n",
    "2. Generated semantic embeddings using SIKU-BERT (specialized for Classical Chinese)\n",
    "3. Calculated pairwise cosine similarity between all edicts\n",
    "4. Visualized the similarity patterns through heatmaps and distributions\n",
    "5. Identified the most similar edicts for content comparison\n",
    "6. Performed clustering analysis to identify groups of related documents\n",
    "\n",
    "The use of SIKU-BERT provides embeddings specifically trained on Classical Chinese texts, which should better capture the semantic relationships in the edicts compared to general-purpose multilingual models.\n",
    "\n",
    "All embeddings and similarity matrices have been saved for future analysis. To analyze a different document type, simply change the `DOCUMENT_TYPE` variable in section 4b and re-run the subsequent cells."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tang_edicts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
